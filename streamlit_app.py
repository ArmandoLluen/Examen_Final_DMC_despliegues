import os
import re
import streamlit as st
from openai import OpenAI, OpenAIError

from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Pinecone
from pinecone import Pinecone as PineconeClient, ServerlessSpec  # type: ignore

# ğŸ” Cargar claves
PINECONE_API_KEY = st.secrets["PINECONE_API_KEY"]
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

if not PINECONE_API_KEY or not OPENAI_API_KEY:
    st.error("âŒ Las claves necesarias no estÃ¡n definidas.")
    st.stop()

# ğŸ§  Inicializar cliente
client = OpenAI(api_key=OPENAI_API_KEY)

# ğŸ¨ Configurar layout
st.set_page_config(page_title="Chat con documentos", layout="centered", page_icon="ğŸ“„")
st.title("ğŸ“„ Asistente basado en tus documentos")

# ğŸ—‚ Inicializar estado
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "retriever" not in st.session_state:
    st.session_state.retriever = None

if "document_uploaded" not in st.session_state:
    st.session_state.document_uploaded = False

if "documents_indexed" not in st.session_state:
    st.session_state.documents_indexed = False

# âš™ï¸ Cachear recursos
@st.cache_resource
def init_embeddings_and_index():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    pc = PineconeClient(api_key=PINECONE_API_KEY)
    index_name = "indice-rag"

    if index_name not in [i["name"] for i in pc.list_indexes()]:
        pc.create_index(
            name=index_name,
            dimension=384,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )

    index = pc.Index(index_name)
    return embeddings, index

embeddings_model, index = init_embeddings_and_index()

# ğŸ“¤ Subida de archivo
uploaded_file = st.file_uploader("Sube un documento PDF", type=["pdf"], disabled=st.session_state.document_uploaded)

if uploaded_file and not st.session_state.document_uploaded:
    os.makedirs("pdf", exist_ok=True)
    file_path = os.path.join("pdf", uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    st.session_state.document_uploaded = True
    st.success("Documento guardado correctamente.")

# ğŸ“š BotÃ³n para indexar documentos
if st.button("Indexar documentos", disabled=st.session_state.documents_indexed):
    with st.spinner("Procesando documentos..."):
        os.makedirs("pdf", exist_ok=True)
        loader = DirectoryLoader("pdf/", glob="**/*.pdf", loader_cls=PyPDFLoader)
        documents = loader.load()

        for doc in documents:
            doc.page_content = " ".join(doc.page_content.split())

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=20,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = splitter.split_documents(documents)

        records = []
        for i, doc in enumerate(chunks):
            source = doc.metadata.get("source", f"doc_{i}")
            safe_id = re.sub(r'[^a-zA-Z0-9_-]', '_', source)

            vector = embeddings_model.embed_query(doc.page_content)
            if len(vector) != 384:
                st.error(f"Vector con dimensiÃ³n incorrecta: {len(vector)}")
                st.stop()

            records.append({
                "id": f"{safe_id}_chunk_{i}",
                "values": vector,
                "metadata": {
                    "text": doc.page_content,
                    "page": doc.metadata.get("page", 0),
                    "source": source
                }
            })

        try:
            batch_size = 50
            for i in range(0, len(records), batch_size):
                batch = records[i:i + batch_size]
                index.upsert(vectors=batch)
        except Exception as e:
            st.error(f"Error al insertar vectores: {e}")
            st.stop()

        st.session_state.retriever = Pinecone(
            index=index,
            embedding=embeddings_model,
            text_key="text"
        ).as_retriever(search_kwargs={"k": 3})

        st.session_state.documents_indexed = True
        st.success("Documentos indexados correctamente.")

# ğŸ§  FunciÃ³n RAG
def responder(question):
    saludos = [
        "hola", "buenas", "buenos dÃ­as", "buenas tardes", "buenas noches",
        "quÃ© tal", "hey", "holi", "holis", "saludos", "hello", "hi"
    ]
    if question.lower().strip() in saludos:
        return "Â¡Hola! ğŸ‘‹ Soy tu asistente experto en documentos. Â¿QuÃ© quieres consultar hoy?"

    if st.session_state.retriever is None:
        return "âš ï¸ Primero debes cargar documentos PDF usando el botÃ³n."

    docs = st.session_state.retriever.invoke(question)

    if not docs:
        prompt_general = f"""
        Eres un asistente experto. Responde de forma clara, precisa y profesional a la siguiente pregunta:

        {question}
        """
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt_general}],
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except OpenAIError as e:
            return f"âŒ Error al generar la respuesta: {str(e)}"
        except Exception as e:
            return f"âŒ Error inesperado: {str(e)}"

    context = "\n\n".join([
        f"(Referencia: {doc.metadata.get('source', 'desconocido')} - pÃ¡gina {doc.metadata.get('page', 'N/A')})\n{doc.page_content}"
        for doc in docs
    ])

    prompt = f"""
    Eres un experto en la documentaciÃ³n proporcionada. Responde de forma clara, precisa y profesional.
    Incluye referencias al documento fuente si es posible. Solo si el contexto no es suficiente, responde con tu conocimiento general.

    Contexto:
    {context}

    Pregunta:
    {question}
    """

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except OpenAIError as e:
        return f"âŒ Error al generar la respuesta: {str(e)}"
    except Exception as e:
        return f"âŒ Error inesperado: {str(e)}"

# ğŸ’¬ Mostrar historial
if st.session_state.chat_history:
    for user_msg, bot_msg in st.session_state.chat_history:
        with st.chat_message("user"):
            st.write(user_msg)
        with st.chat_message("assistant"):
            st.write(bot_msg)

# ğŸ“ Entrada del usuario
if st.session_state.retriever:
    user_input = st.chat_input("Escribe tu pregunta...")

    if user_input:
        with st.chat_message("user"):
            st.write(user_input)

        with st.spinner("Generando respuesta..."):
            respuesta = responder(user_input)

        with st.chat_message("assistant"):
            st.write(respuesta)

        st.session_state.chat_history.append((user_input, respuesta))
else:
    st.info("Sube e indexa tus documentos para comenzar.")
